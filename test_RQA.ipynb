{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs4TVk7xTVYp"
      },
      "source": [
       ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmRpUWabTVYr"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Retrieval Augmentation\n",
        "\n",
        "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
        "\n",
        "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
        "\n",
        "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
        "\n",
        "To begin, we must install the prerequisite libraries that we will be using in this notebook. If we install all libraries we will find a conflict in the Hugging Face `datasets` library so we must install everything in a specific order like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvO12ej5TVYr",
        "outputId": "d4401345-0ab0-43eb-c77f-c68d46d95cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for multiprocess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.293-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.38-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.293 langsmith-0.0.38 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-3.16.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pypdf, pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105 pypdf-3.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    datasets==2.12.0 \\\n",
        "    apache_beam \\\n",
        "    mwparserfromhell\n",
        "!pip install langchain --upgrade\n",
        "!pip install pdfminer.six pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IdEE1fTVYr"
      },
      "source": [
        "## Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiRWzKh0mMGv"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, PyPDFDirectoryLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_folder_path = './pdf'\n",
        "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LarkabZgtbhQ",
        "outputId": "36c277e3-203f-4ebd-8089-68e9a82e6f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='for a rewrite of our production indexing system. Sec-\\ntion 7 discusses related and future work.\\n2 Programming Model\\nThe computation takes a set of input key/value pairs, and\\nproduces a set of output key/value pairs. The user of\\nthe MapReduce library expresses the computation as two\\nfunctions: Map andReduce.\\nMap, written by the user, takes an input pair and pro-\\nduces a set of intermediate key/value pairs. The MapRe-\\nduce library groups together all intermediate values asso-\\nciated with the same intermediate key Iand passes them\\nto the Reduce function.\\nTheReduce function, also written by the user, accepts\\nan intermediate key Iand a set of values for that key. It\\nmerges together these values to form a possibly smaller\\nset of values. Typically just zero or one output value is\\nproduced per Reduce invocation. The intermediate val-\\nues are supplied to the user\\'s reduce function via an iter-\\nator. This allows us to handle lists of values that are too\\nlarge to \\x02t in memory.\\n2.1 Example\\nConsider the problem of counting the number of oc-\\ncurrences of each word in a large collection of docu-\\nments. The user would write code similar to the follow-\\ning pseudo-code:\\nmap(String key, String value):\\n// key: document name\\n// value: document contents\\nfor each word w in value:\\nEmitIntermediate(w, \"1\");\\nreduce(String key, Iterator values):\\n// key: a word\\n// values: a list of counts\\nint result = 0;\\nfor each v in values:\\nresult += ParseInt(v);\\nEmit(AsString(result));\\nThemap function emits each word plus an associated\\ncount of occurrences (just `1\\' in this simple example).\\nThereduce function sums together all counts emitted\\nfor a particular word.\\nIn addition, the user writes code to \\x02ll in a mapreduce\\nspeci\\x02cation object with the names of the input and out-\\nput \\x02les, and optional tuning parameters. The user then\\ninvokes the MapReduce function, passing it the speci\\x02-\\ncation object. The user\\'s code is linked together with the\\nMapReduce library (implemented in C++). Appendix A\\ncontains the full program text for this example.2.2 Types\\nEven though the previous pseudo-code is written in terms\\nof string inputs and outputs, conceptually the map and\\nreduce functions supplied by the user have associated\\ntypes:\\nmap(k1,v1) !list(k2,v2)\\nreduce(k2,list(v2)) !list(v2)\\nI.e., the input keys and values are drawn from a different\\ndomain than the output keys and values. Furthermore,\\nthe intermediate keys and values are from the same do-\\nmain as the output keys and values.\\nOur C++ implementation passes strings to and from\\nthe user-de\\x02ned functions and leaves it to the user code\\nto convert between strings and appropriate types.\\n2.3 More Examples\\nHere are a few simple examples of interesting programs\\nthat can be easily expressed as MapReduce computa-\\ntions.\\nDistributed Grep: The map function emits a line if it\\nmatches a supplied pattern. The reduce function is an\\nidentity function that just copies the supplied intermedi-\\nate data to the output.\\nCount of URL Access Frequency: The map func-\\ntion processes logs of web page requests and outputs\\nhURL;1i. The reduce function adds together all values\\nfor the same URL and emits a hURL;total count i\\npair.\\nReverse Web-Link Graph: The map function outputs\\nhtarget ;source ipairs for each link to a target\\nURL found in a page named source. The reduce\\nfunction concatenates the list of all source URLs as-\\nsociated with a given target URL and emits the pair:\\nhtarget ; list(source )i\\nTerm-Vector per Host: A term vector summarizes the\\nmost important words that occur in a document or a set\\nof documents as a list of hword; frequency ipairs. The\\nmap function emits a hhostname ;term vector i\\npair for each input document (where the hostname is\\nextracted from the URL of the document). The re-\\nduce function is passed all per-document term vectors\\nfor a given host. It adds these term vectors together,\\nthrowing away infrequent terms, and then emits a \\x02nal\\nhhostname ;term vector ipair.\\n2', metadata={'source': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjveir2UTVYs"
      },
      "source": [
        "Now we install the remaining libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_4wHAWtmAvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fcbda1-436d-4434-935a-5786899107e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/770.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/770.9 kB\u001b[0m \u001b[31m869.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/770.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/770.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.0/770.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.9/770.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.50.0 requires protobuf<4.24.0,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.0.162 \\\n",
        "  openai==0.27.7 \\\n",
        "  tiktoken==0.4.0 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8efZWyfMTVYs"
      },
      "source": [
        "---\n",
        "\n",
        "🚨 _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpcO-TwuQwD"
      },
      "source": [
        "Every record contains *a lot* of text. Our first task is therefore to identify a good preprocessing methodology for chunking these articles into more \"concise\" chunks to later be embedding and stored in our Pinecone vector database.\n",
        "\n",
        "For this we use LangChain's `RecursiveCharacterTextSplitter` to split our text into chunks of a specified max length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jdoUJ53TVYs",
        "outputId": "50c51820-b7cb-450e-a3f3-1d9c7c2071a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Encoding 'cl100k_base'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ChSxlcwX8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3640821b-55a9-4c6e-99b4-142a03ffc3b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
        "             \"we can find the length of this chunk of text in tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58J-y6GHtvQP"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=0,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8KGqv-rzEgH"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_documents(data)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9hdjy22zVuJ",
        "outputId": "61384bdb-5267-41cf-c527-27151914d3d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(376, 384, 70)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tiktoken_len(chunks[0].page_content), tiktoken_len(chunks[1].page_content), tiktoken_len(chunks[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvApQNma0K8u"
      },
      "source": [
        "Using the `text_splitter` we get much better sized chunks of text. We'll use this functionality during the indexing process later. Now let's take a look at embedding.\n",
        "\n",
        "## Creating Embeddings\n",
        "\n",
        "Building embeddings using LangChain's OpenAI embedding support is fairly straightforward. We first need to add our [OpenAI api key]() by running the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dphi6CC33p62"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get openai api key from platform.openai.com\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hoj_ZS3wAr"
      },
      "source": [
        "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*\n",
        "\n",
        "After initializing the API key we can initialize our `text-embedding-ada-002` embedding model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBLIWLkLzyGi"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbZGT-v4iMi"
      },
      "source": [
        "Now we embed some text like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM-HuKtl4cyt",
        "outputId": "d6839575-a795-425d-e6f7-7683b7323b29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPUmWYSA43eC"
      },
      "source": [
        "From this we get *two* (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
        "\n",
        "Now we move on to initializing our Pinecone vector database.\n",
        "\n",
        "## Vector Database\n",
        "\n",
        "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "NqWl4f26TVYt"
      },
      "outputs": [],
      "source": [
        "index_name = 'YOUR_PINECONE_INDEX_NAME'\n",
        "# Dimension set to 1536 for GPT_3_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pT9C4nW4vwo"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "# find ENV (cloud region) next to API key in console\n",
        "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key='PINECONE_API_KEY',\n",
        "    environment='PINECONE_ENVIRONMENT'\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgPUwd6REY6z"
      },
      "source": [
        "Then we connect to the new index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFydARw4EcoQ",
        "outputId": "07b3839b-6080-4983-fca9-59b19dfbcdd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.00039,\n",
              " 'namespaces': {'': {'vector_count': 39}},\n",
              " 'total_vector_count': 39}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RqIF2mIDwFu"
      },
      "source": [
        "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
        "\n",
        "## Indexing\n",
        "\n",
        "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "1bd2c99029774512a63f6ea2358747d8",
            "af90c1570e2f43bebbb1bd4852cdbaa3",
            "ed8b2d5c7b6846c9914fc81eb612bf24",
            "38876c3dcc314226ad5eb5af18864653",
            "17d58a6ef0ee4079b68b69d304f86f2a",
            "f12d417c3d874e408e81d9661b3b34ba",
            "4ae5410d7c184aaba82e9dac3335bff2",
            "b20aeb64166249d8b682465b27ce3556",
            "ea06877d00264d4dbf7ba70db4cfc02a",
            "955a7241f1c54d289e335b714cc66035",
            "7f7d9dc7faf2411ab99a7f853f361a8e"
          ]
        },
        "id": "W-cIOoTWGY1R",
        "outputId": "4f12f614-db71-439c-9cb9-3737a36cd6b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bd2c99029774512a63f6ea2358747d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n",
            "pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_limit = 100\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "for i, document in enumerate(tqdm(data)):\n",
        "# for i, document in enumerate(tqdm(final_data)):\n",
        "    # first get metadata fields for this record\n",
        "    # print(i)\n",
        "    # print(type(i))\n",
        "    metadata = {\n",
        "        'source': document.metadata['source']\n",
        "    }\n",
        "    print(document.metadata['source'])\n",
        "    # now we create chunks from the record text\n",
        "    record_texts = text_splitter.split_text(document.page_content)\n",
        "    # create individual metadata dicts for each chunk\n",
        "    record_metadatas = [{\n",
        "        \"chunk\": j, \"text\": text, **metadata\n",
        "    } for j, text in enumerate(record_texts)]\n",
        "    # append these to current batches\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    # if we have reached the batch_limit we can add texts\n",
        "    if len(texts) >= batch_limit:\n",
        "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "        embeds = embed.embed_documents(texts)\n",
        "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "\n",
        "if len(texts) > 0:\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    embeds = embed.embed_documents(texts)\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaF3daSxyCwB"
      },
      "source": [
        "We've now indexed everything. We can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaEBhsAM22M3",
        "outputId": "c182e407-bb12-416b-9398-b23eb65d7068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 0}},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8P2PryCy8W3"
      },
      "source": [
        "## Creating a Vector Store and Querying\n",
        "\n",
        "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMXlvXOAyJHy"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COT5s7hcyPiq",
        "outputId": "4c344f98-0ad4-4c79-a92f-9e2f7658013e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"MapReduce: Simpli\\x02ed Data Processing on Large Clusters\\nJeffrey Dean and Sanjay Ghemawat\\njeff@google.com, sanjay@google.com\\nGoogle, Inc.\\nAbstract\\nMapReduce is a programming model and an associ-\\nated implementation for processing and generating large\\ndata sets. Users specify a map function that processes a\\nkey/value pair to generate a set of intermediate key/value\\npairs, and a reduce function that merges all intermediate\\nvalues associated with the same intermediate key. Many\\nreal world tasks are expressible in this model, as shown\\nin the paper.\\nPrograms written in this functional style are automati-\\ncally parallelized and executed on a large cluster of com-\\nmodity machines. The run-time system takes care of the\\ndetails of partitioning the input data, scheduling the pro-\\ngram's execution across a set of machines, handling ma-\\nchine failures, and managing the required inter-machine\\ncommunication. This allows programmers without any\\nexperience with parallel and distributed systems to eas-\\nily utilize the resources of a large distributed system.\\nOur implementation of MapReduce runs on a large\\ncluster of commodity machines and is highly scalable:\\na typical MapReduce computation processes many ter-\\nabytes of data on thousands of machines. Programmers\\n\\x02nd the system easy to use: hundreds of MapReduce pro-\\ngrams have been implemented and upwards of one thou-\\nsand MapReduce jobs are executed on Google's clusters\\nevery day.\\n1 Introduction\\nOver the past \\x02ve years, the authors and many others at\\nGoogle have implemented hundreds of special-purpose\\ncomputations that process large amounts of raw data,\\nsuch as crawled documents, web request logs, etc., to\\ncompute various kinds of derived data, such as inverted\\nindices, various representations of the graph structure\\nof web documents, summaries of the number of pages\", metadata={'chunk': 0.0, 'source': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf'}),\n",
              " Document(page_content='Google, including:\\n\\x0flarge-scale machine learning problems,\\n\\x0fclustering problems for the Google News and\\nFroogle products,\\n\\x0fextraction of data used to produce reports of popular\\nqueries (e.g. Google Zeitgeist),\\n\\x0fextraction of properties of web pages for new exper-\\niments and products (e.g. extraction of geographi-\\ncal locations from a large corpus of web pages for\\nlocalized search), and\\n\\x0flarge-scale graph computations.2003/032003/062003/092003/122004/032004/062004/09 02004006008001000Number of instances in source tree\\nFigure 4: MapReduce instances over time\\nNumber of jobs 29,423\\nAverage job completion time 634 secs\\nMachine days used 79,186 days\\nInput data read 3,288 TB\\nIntermediate data produced 758 TB\\nOutput data written 193 TB\\nAverage worker machines per job 157\\nAverage worker deaths per job 1.2\\nAverage map tasks per job 3,351\\nAverage reduce tasks per job 55\\nUnique map implementations 395\\nUnique reduce implementations 269\\nUnique map/reduce combinations 426\\nTable 1: MapReduce jobs run in August 2004\\nFigure 4 shows the signi\\x02cant growth in the number of\\nseparate MapReduce programs checked into our primary\\nsource code management system over time, from 0 in\\nearly 2003 to almost 900 separate instances as of late\\nSeptember 2004. MapReduce has been so successful be-\\ncause it makes it possible to write a simple program and\\nrun it ef\\x02ciently on a thousand machines in the course\\nof half an hour, greatly speeding up the development and\\nprototyping cycle. Furthermore, it allows programmers\\nwho have no experience with distributed and/or parallel\\nsystems to exploit large amounts of resources easily.', metadata={'chunk': 1.0, 'source': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf'}),\n",
              " Document(page_content='into a set of Msplits. The input splits can be pro-\\ncessed in parallel by different machines. Reduce invoca-\\ntions are distributed by partitioning the intermediate key\\nspace into Rpieces using a partitioning function (e.g.,\\nhash(key )modR). The number of partitions (R ) and\\nthe partitioning function are speci\\x02ed by the user.\\nFigure 1 shows the overall \\x03ow of a MapReduce op-\\neration in our implementation. When the user program\\ncalls theMapReduce function, the following sequence\\nof actions occurs (the numbered labels in Figure 1 corre-\\nspond to the numbers in the list below):\\n1. The MapReduce library in the user program \\x02rst\\nsplits the input \\x02les into Mpieces of typically 16\\nmegabytes to 64 megabytes (MB) per piece (con-\\ntrollable by the user via an optional parameter). It\\nthen starts up many copies of the program on a clus-\\nter of machines.\\n2. One of the copies of the program is special \\x96 the\\nmaster. The rest are workers that are assigned work\\nby the master. There are Mmap tasks and Rreduce\\ntasks to assign. The master picks idle workers and\\nassigns each one a map task or a reduce task.\\n3. A worker who is assigned a map task reads the\\ncontents of the corresponding input split. It parses\\nkey/value pairs out of the input data and passes each\\npair to the user-de\\x02ned Map function. The interme-\\ndiate key/value pairs produced by the Map function\\nare buffered in memory.\\n4. Periodically, the buffered pairs are written to local\\ndisk, partitioned into Rregions by the partitioning\\nfunction. The locations of these buffered pairs on\\nthe local disk are passed back to the master, who', metadata={'chunk': 0.0, 'source': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "query = \"What is MapReduce?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCvtmREd0pdo"
      },
      "source": [
        "All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA.\n",
        "\n",
        "## Generative Question-Answering\n",
        "\n",
        "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
        "\n",
        "To do this we initialize a `RetrievalQA` object like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moCvQR-p0Zsb"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo', # model name\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "KS9sa19K3LkQ",
        "outputId": "13eea612-9b5c-49d7-eb9b-bd3ff361b656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MapReduce is a programming model and implementation developed by Jeffrey Dean and Sanjay Ghemawat at Google. It is designed for processing and generating large data sets. The concept behind MapReduce is to simplify the processing of big data by allowing users to specify a map function and a reduce function.\\n\\nIn the MapReduce model, users define a map function that takes a key/value pair as input and generates a set of intermediate key/value pairs. The map function processes the input data and performs any necessary transformations or computations. The intermediate key/value pairs are then passed to the reduce function.\\n\\nThe reduce function merges all the intermediate values associated with the same intermediate key. It performs any necessary aggregations or calculations on the intermediate values to produce the final output.\\n\\nThe MapReduce model is highly scalable and can be executed on a large cluster of commodity machines. The runtime system takes care of partitioning the input data, scheduling the program\\'s execution across multiple machines, handling machine failures, and managing inter-machine communication.\\n\\nThe paper \"MapReduce: Simplified Data Processing on Large Clusters\" by Jeffrey Dean and Sanjay Ghemawat provides a detailed explanation of the MapReduce model and its implementation. It discusses the benefits of using MapReduce for processing large data sets and provides examples of real-world tasks that can be expressed using the model.\\n\\nReference:\\nDean, J., & Ghemawat, S. (2004). MapReduce: Simplified Data Processing on Large Clusters. Retrieved from https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "query = \"What is MapReduce? Please explain in detail and provide the references.\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf5e3xf3ggq"
      },
      "source": [
        "We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYVMGDA13cTz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXsVEh3S4ZJO",
        "outputId": "ccc0f23c-56a7-459f-ab35-a029f923245c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is MapReduce?',\n",
              " 'answer': 'MapReduce is a programming model and implementation for processing and generating large data sets. It allows users to specify a map function that processes key/value pairs and generates intermediate key/value pairs, as well as a reduce function that merges intermediate values associated with the same key. MapReduce programs are automatically parallelized and executed on a large cluster of machines. It has been used for various tasks at Google, including large-scale machine learning, clustering, data extraction, and graph computations. The implementation of MapReduce at Google is highly scalable and has processed terabytes of data on thousands of machines. (',\n",
              " 'sources': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf)'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "query = \"What is MapReduce?\"\n",
        "qa_with_sources(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the architecture of MapReduce?\"\n",
        "qa_with_sources(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZp5WOegULCu",
        "outputId": "578020d1-4360-4040-e41a-50af54a78d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the architecture of MapReduce?',\n",
              " 'answer': 'The architecture of MapReduce consists of a programming model and an associated implementation for processing and generating large data sets. It involves a map function that processes a key/value pair to generate intermediate key/value pairs, and a reduce function that merges intermediate values associated with the same key. The implementation runs on a large cluster of commodity machines and is highly scalable. It automatically parallelizes programs written in the functional style and handles partitioning of input data, scheduling of program execution, machine failures, and inter-machine communication. The architecture is described in detail in the paper \"MapReduce: Simplified Data Processing on Large Clusters\" by Jeffrey Dean and Sanjay Ghemawat.\\n',\n",
              " 'sources': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you explain what experiment they produced in this paper?\"\n",
        "qa_with_sources(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBFvYK3aUcs9",
        "outputId": "6dd0c8c6-e188-4830-d370-e78b5af3b509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Can you explain what experiment they produced in this paper?',\n",
              " 'answer': 'The paper discusses the implementation and performance measurements of the MapReduce programming model in a cluster-based computing environment. It also explores the use of MapReduce within Google for various tasks, including large-scale machine learning problems, clustering problems, data extraction, and large-scale graph computations. The paper provides examples of the effect of backup tasks and machine failures on the execution of the sort program using MapReduce. Additionally, it presents statistics on the computational resources used by MapReduce jobs at Google, including the number of jobs, average completion time, machine days used, input data read, intermediate data produced, output data written, and average number of worker machines per job. \\n',\n",
              " 'sources': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you explain what does map and reduce phases do in Mapreduce?\"\n",
        "qa_with_sources(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwW0qtgG44eV",
        "outputId": "cc3d5726-4159-4086-df2b-569de1ab5cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Can you explain what does map and reduce phases do in Mapreduce?',\n",
              " 'answer': \"In the MapReduce framework, the map phase involves processing a key/value pair to generate a set of intermediate key/value pairs. The reduce phase then merges all intermediate values associated with the same intermediate key. This allows for parallel processing of large data sets on a cluster of machines. The map and reduce functions are specified by the user and are used to express the computation. The map function emits intermediate key/value pairs, while the reduce function merges the values for each key. The MapReduce framework takes care of partitioning the input data, scheduling the program's execution, handling machine failures, and managing inter-machine communication. This allows programmers without experience in parallel and distributed systems to utilize the resources of a large distributed system. The implementation of MapReduce at Google runs on a large cluster of commodity machines and is highly scalable. It can process many terabytes of data on thousands of machines. [\",\n",
              " 'sources': 'pdf/MapReduce- Simplified Data Processing on Large Clusters.pdf]'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRk_P3Q7l5J"
      },
      "source": [
        "Now we answer the question being asked, *and* return the source of this information being used by the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehJEn68qADoH"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bd2c99029774512a63f6ea2358747d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af90c1570e2f43bebbb1bd4852cdbaa3",
              "IPY_MODEL_ed8b2d5c7b6846c9914fc81eb612bf24",
              "IPY_MODEL_38876c3dcc314226ad5eb5af18864653"
            ],
            "layout": "IPY_MODEL_17d58a6ef0ee4079b68b69d304f86f2a"
          }
        },
        "af90c1570e2f43bebbb1bd4852cdbaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12d417c3d874e408e81d9661b3b34ba",
            "placeholder": "​",
            "style": "IPY_MODEL_4ae5410d7c184aaba82e9dac3335bff2",
            "value": "100%"
          }
        },
        "ed8b2d5c7b6846c9914fc81eb612bf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b20aeb64166249d8b682465b27ce3556",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea06877d00264d4dbf7ba70db4cfc02a",
            "value": 13
          }
        },
        "38876c3dcc314226ad5eb5af18864653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_955a7241f1c54d289e335b714cc66035",
            "placeholder": "​",
            "style": "IPY_MODEL_7f7d9dc7faf2411ab99a7f853f361a8e",
            "value": " 13/13 [00:00&lt;00:00, 136.97it/s]"
          }
        },
        "17d58a6ef0ee4079b68b69d304f86f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f12d417c3d874e408e81d9661b3b34ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae5410d7c184aaba82e9dac3335bff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b20aeb64166249d8b682465b27ce3556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea06877d00264d4dbf7ba70db4cfc02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "955a7241f1c54d289e335b714cc66035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7d9dc7faf2411ab99a7f853f361a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
